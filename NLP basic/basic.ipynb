{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Folding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, welcome to my youtube channel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, Welcome to my Youtube channel\"\n",
    "text=text.casefold()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, welcome to my youtube channel'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCR\n",
    "@ # $ % ^ & "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ram is going \n",
    "# Ram@ is going& \n",
    "#  noise reduction \n",
    "# improved text quality\n",
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "# A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern.\n",
    "\n",
    "# RegEx can be used to check if a string contains the specified search pattern.\n",
    "\n",
    "# RegEx Module\n",
    "# Python has a built-in package called re, which can be used to work with Regular Expressions.\n",
    "\n",
    "# Import the re module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"hello@ my name is #govind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_str=re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name is govind'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name123 is govind'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"hello@ my name123 is #govind\"\n",
    "clean_str=re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",input)\n",
    "clean_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libearayies in the field of NLP \n",
    "     Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"hello@ my name123 is #govind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name is govind\n"
     ]
    }
   ],
   "source": [
    "def clean_txt(text):\n",
    "    cleaned_txt=''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "    doc=nlp(cleaned_txt)\n",
    "    return ' '.join(token.text for token in doc)\n",
    "\n",
    "clean_str=clean_txt(input)\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BY using nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '@', 'my', 'name123', 'is', '#', 'govind']\n",
      "hello my name123 is govind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kumaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "input = \"hello@ my name123 is #govind\"\n",
    "# tokenize\n",
    "tokens=nltk.word_tokenize(input)\n",
    "print(tokens)\n",
    "\n",
    "# remove the special characters \n",
    "clain_token=[token for token in tokens if token.isalnum()]\n",
    "clean_str = ' '.join(clain_token)\n",
    "print(clean_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling contractions \n",
    "I`m = i am\n",
    "\n",
    "\n",
    "\n",
    "i`m going "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am govind\n"
     ]
    }
   ],
   "source": [
    "input =\" i'm govind\"\n",
    "expanded_text=contractions.fix(input)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re \n",
    "# def expand_contraction():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=word_tokenize(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"'m\", 'govind']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"An introduction serves as the opening section of any document, presentation, or discussion, setting the stage for the content that follows. It typically provides an overview of the topic, establishes its context, and highlights its importance or relevance. A well-crafted introduction grabs the readerâ€™s or audienceâ€™s attention, clearly outlines the purpose or main idea, and often includes a brief roadmap of what to expect.\"\n",
    "sent_output =sent_tokenize(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An introduction serves as the opening section of any document, presentation, or discussion, setting the stage for the content that follows.',\n",
       " 'It typically provides an overview of the topic, establishes its context, and highlights its importance or relevance.',\n",
       " 'A well-crafted introduction grabs the readerâ€™s or audienceâ€™s attention, clearly outlines the purpose or main idea, and often includes a brief roadmap of what to expect.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "intro = \"Hello my name is Govind kumawat i am from jaipur \"\n",
    "intro=word_tokenize(intro)\n",
    "newsentence=[word for word in intro if word.lower() not in stopwords.words('english')]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'Govind', 'kumawat', 'i', 'am', 'from', 'jaipur']\n",
      "['Hello', 'name', 'Govind', 'kumawat', 'jaipur']\n"
     ]
    }
   ],
   "source": [
    "print(intro)\n",
    "print(newsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_gram (unigrams , bigrams, trigrams)\n",
    "An N-gram is a contiguous sequence of \n",
    "ð‘›\n",
    "n items (words, characters, or other tokens) from a given text or speech dataset.\n",
    "\n",
    "* Unigram: Single words (e.g., [\"The\", \"cat\", \"sits\"])\n",
    "* Bigram: Pairs of words (e.g., [\"The cat\", \"cat sits\"])\n",
    "* Trigram: Triplets of words (e.g., [\"The cat sits\"])\n",
    "N-gram: A general sequence of \n",
    "ð‘›\n",
    "n words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkit: Package 'punkit' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello my name\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello',), ('my',), ('name',)]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram(text ,n ):\n",
    "    tokens=word_tokenize(text)\n",
    "    ngrams_list= list(ngrams(tokens,n))\n",
    "    return ngrams_list\n",
    "unigram = generate_ngram(text=text,n=1)\n",
    "bigram = generate_ngram(text=text,n=2)\n",
    "triigram = generate_ngram(text=text,n=3)\n",
    "print(unigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('This',), ('is',), ('a',), ('sample',), ('text',), ('for',), ('testing',), ('n-grams',), ('.',)]\n",
      "Bigrams: [('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'text'), ('text', 'for'), ('for', 'testing'), ('testing', 'n-grams'), ('n-grams', '.')]\n",
      "Trigrams: [('This', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'text'), ('sample', 'text', 'for'), ('text', 'for', 'testing'), ('for', 'testing', 'n-grams'), ('testing', 'n-grams', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_ngram(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    ngram_list = list(ngrams(tokens, n))  # Use a different variable name\n",
    "    return ngram_list\n",
    "\n",
    "text = \"This is a sample text for testing n-grams.\"\n",
    "unigram = generate_ngram(text=text, n=1)\n",
    "bigram = generate_ngram(text=text, n=2)\n",
    "trigram = generate_ngram(text=text, n=3)\n",
    "\n",
    "print(\"Unigrams:\", unigram)\n",
    "print(\"Bigrams:\", bigram)\n",
    "print(\"Trigrams:\", trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorization of text data \n",
    "Common Text Vectorization Techniques\n",
    "1. Count Vectorization\n",
    "Converts text into a matrix of token counts.\n",
    "Example:\n",
    "Input text: [\"I like NLP\", \"I like Python\"]\n",
    "Vocabulary: {\"I\": 2, \"like\": 2, \"NLP\": 1, \"Python\": 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings \n",
    "word------->> vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words(BoW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
